{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4R1UHuc6KCl5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "wI1g3yxzKM7F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "  \"\"\"\n",
        "    Process tweet function:\n",
        "    Input:\n",
        "      tweet: a string contain a tweet\n",
        "    Output:\n",
        "      tweets_clean: a list of words containing the processed tweets\n",
        "  \"\"\"\n",
        "\n",
        "  stemmer= PorterStemmer()\n",
        "  stopwords_english= stopwords.words('english')\n",
        "\n",
        "  #remove stoch market tickers like $GE\n",
        "  tweet= re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "  #remove old style retweet text \"RT\"\n",
        "  tweet= re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  #remove hyperlinks\n",
        "  tweet= re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "\n",
        "  #remove hashtags\n",
        "  #only removing the hash # sign from the word\n",
        "  tweet= re.sub(r'#', '', tweet)\n",
        "\n",
        "  #tokenize tweets\n",
        "  tokenizer= TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "  tweet_tokens= tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweets_clean= []\n",
        "  for word in tweet_tokens:\n",
        "    if(word not in stopwords_english and #remove stopwords\n",
        "       word not in string.punctuation): # remove puntuation\n",
        "       #tweets_clean.append(word)\n",
        "       stem_word= stemmer.stem(word) #stemming (root)\n",
        "       tweets_clean.append(stem_word)\n",
        "\n",
        "  return tweets_clean\n",
        "\n"
      ],
      "metadata": {
        "id": "szgQHLvBKcQj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "  \"\"\"\n",
        "    Buikd frequencies:\n",
        "    Input:\n",
        "      tweets: a list of tweets\n",
        "      ys: an m x 1 array with sentiment label of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "      freqs: a dictionary mapping each (word, sentiment) pair to its requency.\n",
        "  \"\"\"\n",
        "  # convert np array to list since zip needs an iterable.\n",
        "  # the squeeze is necessary or the list ends up with one element.\n",
        "  # also this is just a NOP it ys is aleady a list.\n",
        "  yslist= np.squeeze(ys).tolist()\n",
        "\n",
        "  # start with an empty dictionary and populate it by looping over all tweets.\n",
        "  # and over all processed words in each tweet.\n",
        "  freqs= {}\n",
        "  for y, tweet in zip(yslist, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] +=1\n",
        "      else:\n",
        "        freqs[pair] = 1\n",
        "\n",
        "  return freqs\n"
      ],
      "metadata": {
        "id": "SrU_k7-BSUBM"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}