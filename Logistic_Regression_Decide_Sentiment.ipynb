{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##  Logistic Regression\n",
        "will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, we will decide if it has a positive sentiment or a negative one.\n",
        "\n",
        "### Specifically we will:\n",
        "- Learn how to extract features for logistic regression given some text\n",
        "- Implement logistic regression from scratch\n",
        "- Apply logistic regression on a natural language processing task\n",
        "- Test using your logistic regression\n",
        "- Perform error analysis"
      ],
      "metadata": {
        "id": "1rZFKX7xOG5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE3CYemsAZ9E",
        "outputId": "fe8a6b98-5071-498a-cd92-e9a331545398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# import Functions and Data\n",
        "import nltk\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filePath= f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ],
      "metadata": {
        "id": "SNTKQkycO02M"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "fmCxAjF5PPn3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "  \"\"\"\n",
        "    Process tweet function:\n",
        "    Input:\n",
        "      tweet: a string contain a tweet\n",
        "    Output:\n",
        "      tweets_clean: a list of words containing the processed tweets\n",
        "  \"\"\"\n",
        "\n",
        "  stemmer= PorterStemmer() # root\n",
        "  stopwords_english= stopwords.words('english')\n",
        "\n",
        "  #remove stoch market tickers like $GE\n",
        "  tweet= re.sub(r'$\\w*', '', tweet)\n",
        "\n",
        "  #remove old style retweet text \"RT\"\n",
        "  tweet= re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  #remove hyperlinks\n",
        "  tweet= re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "\n",
        "  #remove hashtags\n",
        "  #only removing the hash # sign from the word\n",
        "  tweet= re.sub(r'#', '', tweet)\n",
        "\n",
        "  #tokenize tweets\n",
        "  tokenizer= TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "  tweet_tokens= tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweets_clean= []\n",
        "  for word in tweet_tokens:\n",
        "    if(word not in stopwords_english and #remove stopwords\n",
        "       word not in string.punctuation): # remove puntuation\n",
        "       #tweets_clean.append(word)\n",
        "       stem_word= stemmer.stem(word) #stemming (root)\n",
        "       tweets_clean.append(stem_word)\n",
        "\n",
        "  return tweets_clean"
      ],
      "metadata": {
        "id": "BtV0dvNKP1nJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "  \"\"\"\n",
        "    Build frequencies:\n",
        "    Input:\n",
        "      tweets: a list of tweets\n",
        "      ys: an m x 1 array with sentiment label of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "      freqs: a dictionary mapping each (word, sentiment) pair to its requency.\n",
        "  \"\"\"\n",
        "  # convert np array to list since zip needs an iterable.\n",
        "  # the squeeze is necessary or the list ends up with one element.\n",
        "  # also this is just a NOP it ys is aleady a list.\n",
        "  yslist= np.squeeze(ys).tolist() # squeeze to remove the unneeded dimentions\n",
        "\n",
        "  # start with an empty dictionary and populate it by looping over all tweets.\n",
        "  # and over all processed words in each tweet.\n",
        "  freqs= {}\n",
        "  for y, tweet in zip(yslist, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] +=1\n",
        "      else:\n",
        "        freqs[pair] = 1\n",
        "\n",
        "  return freqs\n"
      ],
      "metadata": {
        "id": "zH9LCsU-P4VP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the Data\n",
        "The twitter_samples contains subsets of five thousand positive_tweets, five thousand negative_tweets"
      ],
      "metadata": {
        "id": "5PRg0_HWTfm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets= twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets= twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "Ay2vt-WTTroV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Train test split: 20% will be in the test set, and 80% in the reaining set."
      ],
      "metadata": {
        "id": "oIcT7Z2uT9PW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into 2 pieces, one for training and one for testing(validation set)\n",
        "test_pos= all_positive_tweets[4000:]\n",
        "train_pos= all_positive_tweets[:4000]\n",
        "\n",
        "test_neg= all_negative_tweets[4000:]\n",
        "train_neg= all_negative_tweets[:4000]\n",
        "\n",
        "train_x= train_pos + train_neg\n",
        "test_x= test_pos + test_neg"
      ],
      "metadata": {
        "id": "Wtw2qSmHT6is"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create the numpy array of positive labels and negative labels."
      ],
      "metadata": {
        "id": "Pkm02yVNU0U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y =np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "PcT8e6YzUtJI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the shape of train and test sets\n",
        "print('train_y.shape= ' + str(train_y.shape))\n",
        "print('test_y.shape= '+ str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7EaFOOuV94r",
        "outputId": "6c5832c6-30f1-49d5-f1ee-a87f624c9d62"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_y.shape= (8000, 1)\n",
            "test_y.shape= (2000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The 'freqs' dictionary is the frequency dictionary that's being built.\n",
        "- The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0). The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
      ],
      "metadata": {
        "id": "A6WjtbAIaB2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create frequency dictionary\n",
        "freqs= build_freqs(train_x, train_y)\n",
        "\n",
        "# check the output\n",
        "print('type(freqs)= '+str(type(freqs)))\n",
        "print('len(freqs)= '+ str(len(freqs.keys())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUoFQdUIZ1-4",
        "outputId": "42a01387-dd62-46f1-828c-cc4b63884e79"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs)= <class 'dict'>\n",
            "len(freqs)= 11396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Tweets"
      ],
      "metadata": {
        "id": "58dnBBenc6vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function below\n",
        "print('this is an example of a positive tweet: \\n', train_x[0])\n",
        "print('\\nthis is an example of the processed version of the tweet:\\n', process_tweet(train_x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xarvh1eEcqB6",
        "outputId": "87bf3f6a-40bd-48e7-ffc1-677aaae2b8d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is an example of a positive tweet: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "this is an example of the processed version of the tweet:\n",
            " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid"
      ],
      "metadata": {
        "id": "7aPn3Gtqd8M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    z: is the input (can be a scalar or an array)\n",
        "  Output:\n",
        "    h: the sigmoid of z\n",
        "  \"\"\"\n",
        "\n",
        "  #calculate the sigmoid of z\n",
        "  h= 1/(1+np.exp(-z))\n",
        "\n",
        "  return h"
      ],
      "metadata": {
        "id": "2iuKrqYRd4lO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the functions\n",
        "if(sigmoid(0)==0.5):\n",
        "  print('Success!')\n",
        "else:\n",
        "  print('Oops!')\n",
        "\n",
        "if(sigmoid(4.92) == 0.9927537604041685):\n",
        "  print('Correct!')\n",
        "else:\n",
        "  print('Oops again!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBdfCM2rcpi4",
        "outputId": "0419e421-fb28-49d4-81be-58976632a3a7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "Correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cost function and Gradient\n",
        "\n",
        "Implement gradient descent function.\n",
        "\n",
        "- The number of iterations 'num_iters\" is the number of times that you'll use the entire training set.\n",
        "- For each iteration, you'll calculate the cost function using all training examples (there are 'm' training examples), and for all features.\n",
        "- Instead of updating a single weight  ðœƒð‘–\n",
        "  at a time, we can update all the weights in the column vector:"
      ],
      "metadata": {
        "id": "HgWBYutIjOgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "      x: matrix of features which is (m, n+1)\n",
        "      y: corresponding labels of the input matrix x, dimensions (m, 1)\n",
        "      theta: weight vector of dimensions (n+1, 1)\n",
        "      num_iters: number of iterations you want to train your model for\n",
        "\n",
        "    Output:\n",
        "      j: the final cost\n",
        "      theta: your final weight vector\n",
        "  \"\"\"\n",
        "  # get 'm', thenumber of rows in matrix x\n",
        "  m = x.shape[0]\n",
        "  for i in range(0, num_iters):\n",
        "\n",
        "    # get z, the dot product of x and theta\n",
        "    z= np.dot(x, theta)\n",
        "\n",
        "    #get the sigmoid of z\n",
        "    h= 1/ (1+ np.exp(-z))\n",
        "\n",
        "    # calculate teh cost functions\n",
        "    J = -1/m * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
        "\n",
        "    # update the weights theta\n",
        "    theta= theta - (alpha / m) * (np.dot(x.T, (h-y)))\n",
        "\n",
        "  J= float(J.item())\n",
        "  return J, theta"
      ],
      "metadata": {
        "id": "M9ekr-8_i7IJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### extracting the Features\n",
        "- Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
        "  - The first feature is the number of positive words in a tweet.\n",
        "  - The second feature is the number of negative words in a tweet.\n",
        "- Then train your logistic regression classifier on these features.\n",
        "- Test the classifier on a validation set."
      ],
      "metadata": {
        "id": "T6ORFRo5pcP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "      tweet: a string containing one tweet\n",
        "      freqs: a dictionary corresponding to the frequencies of each tuple(word, label)\n",
        "    Output:\n",
        "      x: a feature vector of dimension (1, 3)\n",
        "  \"\"\"\n",
        "  # process_tweet tokenizers, stems, and removes stopwords\n",
        "  word_list= process_tweet(tweet)\n",
        "\n",
        "  # 3 elements for [bias, positive, negative] counts\n",
        "  x= np.zeros(3)\n",
        "\n",
        "  # bias term is set to 1\n",
        "  x[0] = 1\n",
        "\n",
        "  # loop through each word in the list of words\n",
        "  for word in word_list:\n",
        "\n",
        "    # increase the word count for the positive label 1\n",
        "    x[1]+= freqs.get((word, 1), 0)\n",
        "\n",
        "    # increase the word count for the negative label 0\n",
        "    x[2]+= freqs.get((word, 0), 0)\n",
        "\n",
        "  x= x[None, :]\n",
        "  assert(x.shape == (1, 3))\n",
        "  return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JBg2IWGVpKMb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the function\n",
        "# test on training data\n",
        "\n",
        "temp1= extract_features(train_x[0], freqs)\n",
        "temp1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh8vIk7rrdy_",
        "outputId": "1dc62bf3-1b1c-432b-f6e2-7df55ebf8e0f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.000e+00, 3.133e+03, 6.100e+01]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test 2:\n",
        "# check for when the words are not in the freqs dictionary\n",
        "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
        "print(tmp2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L56_o7g0ron4",
        "outputId": "3c549855-00d4-4839-ab7f-f2966cd1f982"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model\n",
        "To train the model:\n",
        "\n",
        "- Stack the features for all training examples into a matrix X.\n",
        "- Call gradientDescent, which we've implemented above."
      ],
      "metadata": {
        "id": "K6Trxm9Ixy8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect the features 'x' and stack them into a matrix 'X'\n",
        "X = np.zeros((len(train_x), 3))\n",
        "\n",
        "for i in range(len(train_x)):\n",
        "  X[i, :]= extract_features(train_x[i], freqs)\n",
        "\n",
        "# training labels corresponding to X\n",
        "Y = train_y\n",
        "\n",
        "# apply gradient descent\n",
        "J, theta= gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print(f\"The cost after training is {J:.8f}.\")\n",
        "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\");\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ydhO2HMx9_T",
        "outputId": "c5ceef53-51ad-4f3c-9a25-2779f928e253"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost after training is 0.22524456.\n",
            "The resulting vector of weights is [np.float64(6e-08), np.float64(0.00053786), np.float64(-0.00055885)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict Tweet\n",
        "Implement predict_tweet. Predict whether a tweet is positive or negative.\n",
        "\n",
        "- Given a tweet, process it, then extract the features.\n",
        "- Apply the model's learned weights on the features to get the logits.\n",
        "- Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n"
      ],
      "metadata": {
        "id": "0pZtoyTS5Jz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "      tweet: a string\n",
        "      freqs: a dictionary corresponding to the frequencies of each tuple(word, label)\n",
        "      theta: (3, 1) vector of weights\n",
        "    Output:\n",
        "      y_pred: the probability of a tweet being positive or negative\n",
        "  \"\"\"\n",
        "  # extract the features of the tweet and store it in x\n",
        "  x= extract_features(tweet, freqs)\n",
        "\n",
        "  # make the prediction using x and theta\n",
        "  y_pred= sigmoid(np.dot(x, theta)) # shape: (1, 1)\n",
        "\n",
        "  return y_pred\n"
      ],
      "metadata": {
        "id": "BhxzkXb25ocD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the function\n",
        "for tweet in [\"I am happy\", \"I am bad\", \"this movie should have been great.\", \"great\", \"great great\",'great great great', \"great great great grea\"]:\n",
        "  print(\"%s -> %f\" %(tweet, predict_tweet(tweet, freqs, theta).item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCVUJq_V_aFb",
        "outputId": "ab69fb96-6e55-4502-c24a-76f2a730a62b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy -> 0.519259\n",
            "I am bad -> 0.494338\n",
            "this movie should have been great. -> 0.515962\n",
            "great -> 0.516052\n",
            "great great -> 0.532070\n",
            "great great great -> 0.548023\n",
            "great great great grea -> 0.548023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the Performance using Test Set\n",
        "\n",
        "Implement test_logistic_regression.\n",
        "\n",
        "- Given the test data and the weights of my trained model, calculate the accuracy of my logistic regression model.\n",
        "- Use your 'predict_tweet' function to make predictions on each tweet in the test set.\n",
        "- If the prediction is > 0.5, set the model's classification 'y_hat' to 1, otherwise set the model's classification 'y_hat' to 0.\n",
        "- A prediction is accurate when the y_hat equals the test_y. Sum up all the instances when they are equal and divide by m."
      ],
      "metadata": {
        "id": "-c8o1EaQAfqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "      test_x: a list of tweets\n",
        "      test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "      freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "      theta: weight vector of dimension(3, 1)\n",
        "    Output:\n",
        "      accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "  \"\"\"\n",
        "\n",
        "  #strating the list for storing predictions\n",
        "  y_hat=[]\n",
        "\n",
        "  for tweet in test_x:\n",
        "    #get the label prediction for the tweet\n",
        "    y_pred= predict_tweet(tweet, freqs, theta)\n",
        "\n",
        "    if y_pred > 0.5:\n",
        "      #append 1.0 to the list\n",
        "      y_hat.append(1.0)\n",
        "    else:\n",
        "      #append 0 to the list\n",
        "      y_hat.append(0.0)\n",
        "\n",
        "  # with the above implementation, y_hat is a list, but test_y is (m, 1) array\n",
        "  y_hat= np.array(y_hat)\n",
        "  test_y= np.squeeze(test_y) # or test_y.reshape(-1)\n",
        "\n",
        "  #convert both to one_dimensional arrays in order to compare them using the '=' operator\n",
        "  accuracy= np.sum(y_hat== test_y) / len(test_x)\n",
        "\n",
        "  return accuracy\n"
      ],
      "metadata": {
        "id": "PP5gzziiAX-D"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "651LTCP8TGJE",
        "outputId": "e298980e-fb4c-4eb2-9975-cceb9739cd47"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Error Analysis\n",
        "In this part we will see some tweets that our model misclassified. Why do you think the misclassifications happened? Specifically what kind of tweets does our model misclassify?\n",
        "\n"
      ],
      "metadata": {
        "id": "LDdYhA4NTLPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some error analysis done for you\n",
        "print('Label Predicted Tweet')\n",
        "for x,y in zip(test_x,test_y):\n",
        "    y_hat = predict_tweet(x, freqs, theta)\n",
        "\n",
        "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
        "        print('THE TWEET IS:', x)\n",
        "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
        "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ4yWR_pTeOM",
        "outputId": "3122bdd0-0a32-4a83-adea-bf15f983ac9a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Predicted Tweet\n",
            "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
            "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
            "1\t0.48899230\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-6eb8e65380e7>:9: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE TWEET IS: off to the park to get some sunlight : )\n",
            "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
            "1\t0.49632433\tb'park get sunlight'\n",
            "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
            "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
            "1\t0.48246197\tb'uff itna miss karhi thi ap :p'\n",
            "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
            "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
            "0\t0.50983764\tb'u prob fun david'\n",
            "THE TWEET IS: pats jay : (\n",
            "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
            "0\t0.50040341\tb'pat jay'\n",
            "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
            "0\t0.50000001\tb'belov grandmoth'\n",
            "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
            "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
            "0\t0.50647821\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict with our own Tweet\n"
      ],
      "metadata": {
        "id": "4uI2uNKN-1Hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tweet= \"This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!\"\n",
        "print(process_tweet(my_tweet))\n",
        "y_hat= predict_tweet(my_tweet, freqs, theta)\n",
        "print(y_hat)\n",
        "if y_hat > 0.5:\n",
        "  print('Positive sentiment')\n",
        "else:\n",
        "  print('Negative sentiment')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMYSEjZzTnFw",
        "outputId": "bce30b96-3fa0-4fb1-a9f0-1eee336dae3d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
            "[[0.48122783]]\n",
            "Negative sentiment\n"
          ]
        }
      ]
    }
  ]
}